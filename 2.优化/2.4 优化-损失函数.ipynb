{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 均方误差mse: loss_mse = tf.reduce_mean(tf.square(y_-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例：预测酸奶日销量y，x1和x2是影像日销量的因素。\n",
    "### 拟造数据集X,Y_: y_ = x1 + x2 噪声: -0.05 ~ + 0.05 拟合可以预测销量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23455)\n",
    "x = np.random.random((32, 2))\n",
    "np.random.seed(23455)\n",
    "y_ = [[x1 + x2 + (np.random.random()/10 -0.05)] for (x1, x2) in x]\n",
    "x = tf.cast(x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-0.8113182],\n",
      "       [ 1.4845988]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random.normal([2,1], stddev=1,seed=1))\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 15000\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.6589284, shape=(), dtype=float32)\n",
      "[[-0.80962235]\n",
      " [ 1.4855162 ]] \n",
      "\n",
      "After 500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.23354581, shape=(), dtype=float32)\n",
      "[[-0.21869414]\n",
      " [ 1.6985074 ]] \n",
      "\n",
      "After 1000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.13365382, shape=(), dtype=float32)\n",
      "[[0.09044284]\n",
      " [1.6730996 ]] \n",
      "\n",
      "After 1500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.08748729, shape=(), dtype=float32)\n",
      "[[0.2850088]\n",
      " [1.5850053]] \n",
      "\n",
      "After 2000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.058802005, shape=(), dtype=float32)\n",
      "[[0.42477268]\n",
      " [1.4901432 ]] \n",
      "\n",
      "After 2500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.039770197, shape=(), dtype=float32)\n",
      "[[0.53275084]\n",
      " [1.4047352 ]] \n",
      "\n",
      "After 3000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.026995476, shape=(), dtype=float32)\n",
      "[[0.619081 ]\n",
      " [1.3321261]] \n",
      "\n",
      "After 3500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.018403728, shape=(), dtype=float32)\n",
      "[[0.68914163]\n",
      " [1.2717098 ]] \n",
      "\n",
      "After 4000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.01262338, shape=(), dtype=float32)\n",
      "[[0.74635637]\n",
      " [1.2218647 ]] \n",
      "\n",
      "After 4500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00873426, shape=(), dtype=float32)\n",
      "[[0.7932026]\n",
      " [1.1808821]] \n",
      "\n",
      "After 5000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.006117559, shape=(), dtype=float32)\n",
      "[[0.8315997]\n",
      " [1.1472327]] \n",
      "\n",
      "After 5500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0043569747, shape=(), dtype=float32)\n",
      "[[0.86308604]\n",
      " [1.1196209 ]] \n",
      "\n",
      "After 6000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0031724046, shape=(), dtype=float32)\n",
      "[[0.8889098]\n",
      " [1.0969682]] \n",
      "\n",
      "After 6500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0023753908, shape=(), dtype=float32)\n",
      "[[0.91009074]\n",
      " [1.0783855 ]] \n",
      "\n",
      "After 7000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0018391438, shape=(), dtype=float32)\n",
      "[[0.9274643]\n",
      " [1.0631427]] \n",
      "\n",
      "After 7500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0014783423, shape=(), dtype=float32)\n",
      "[[0.9417151]\n",
      " [1.0506394]] \n",
      "\n",
      "After 8000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0012355894, shape=(), dtype=float32)\n",
      "[[0.9534043]\n",
      " [1.0403837]] \n",
      "\n",
      "After 8500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0010722603, shape=(), dtype=float32)\n",
      "[[0.96299225]\n",
      " [1.0319713 ]] \n",
      "\n",
      "After 9000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00096236827, shape=(), dtype=float32)\n",
      "[[0.970857 ]\n",
      " [1.0250711]] \n",
      "\n",
      "After 9500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00088843016, shape=(), dtype=float32)\n",
      "[[0.977308 ]\n",
      " [1.0194112]] \n",
      "\n",
      "After 10000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0008386794, shape=(), dtype=float32)\n",
      "[[0.98259956]\n",
      " [1.0147681 ]] \n",
      "\n",
      "After 10500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00080520986, shape=(), dtype=float32)\n",
      "[[0.98693997]\n",
      " [1.0109602 ]] \n",
      "\n",
      "After 11000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0007826871, shape=(), dtype=float32)\n",
      "[[0.9905002]\n",
      " [1.0078363]] \n",
      "\n",
      "After 11500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0007675348, shape=(), dtype=float32)\n",
      "[[0.9934208]\n",
      " [1.0052744]] \n",
      "\n",
      "After 12000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00075734046, shape=(), dtype=float32)\n",
      "[[0.9958161]\n",
      " [1.0031729]] \n",
      "\n",
      "After 12500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0007504815, shape=(), dtype=float32)\n",
      "[[0.9977809]\n",
      " [1.0014491]] \n",
      "\n",
      "After 13000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.0007458647, shape=(), dtype=float32)\n",
      "[[0.9993923]\n",
      " [1.0000341]] \n",
      "\n",
      "After 13500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00074276165, shape=(), dtype=float32)\n",
      "[[1.0007129]\n",
      " [0.998875 ]] \n",
      "\n",
      "After 14000 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00074067013, shape=(), dtype=float32)\n",
      "[[1.0017986 ]\n",
      " [0.99792385]] \n",
      "\n",
      "After 14500 training step, w1 is:\n",
      "Loss:  tf.Tensor(0.00073926325, shape=(), dtype=float32)\n",
      "[[1.0026885 ]\n",
      " [0.99714345]] \n",
      "\n",
      "Final w1 is:  [[1.0034136 ]\n",
      " [0.99650455]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.matmul(x, w1)\n",
    "        loss_mse = tf.reduce_mean(tf.square(y_-y))\n",
    "    grads = tape.gradient(loss_mse, w1)\n",
    "    w1.assign_sub(lr * grads)\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(\"After {} training step, w1 is:\".format(epoch))\n",
    "        print(\"Loss: \", loss_mse)\n",
    "        print(w1.numpy(), \"\\n\")\n",
    "print(\"Final w1 is: \", w1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 自定义损失函数\n",
    "### 预测酸奶销量，酸奶成本(COST)1元，酸奶利润(PROFIT00)99元。\n",
    "### 预测少了损失利润99元，大于预测多了损失成本1元，所以预测少了损失大，希望生成的预测函数往多了预测\n",
    "### loss_zdy = tf.reduce_sum(tf.where(tf.greater(y,y_), COST * (y-y_), PROFIT * (y_-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COST = 1\n",
    "PROFIT = 99\n",
    "epoch = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step, w1 is:\n",
      "[[2.5675967]\n",
      " [2.4931712]] \n",
      "\n",
      "After 500 training step, w1 is:\n",
      "[[1.1613214]\n",
      " [1.1071346]] \n",
      "\n",
      "After 1000 training step, w1 is:\n",
      "[[1.3091749]\n",
      " [1.1007752]] \n",
      "\n",
      "After 1500 training step, w1 is:\n",
      "[[1.0584906]\n",
      " [1.0990032]] \n",
      "\n",
      "After 2000 training step, w1 is:\n",
      "[[1.0572413]\n",
      " [1.0666682]] \n",
      "\n",
      "After 2500 training step, w1 is:\n",
      "[[1.085192 ]\n",
      " [1.0966698]] \n",
      "\n",
      "After 3000 training step, w1 is:\n",
      "[[1.2186614]\n",
      " [1.3010343]] \n",
      "\n",
      "After 3500 training step, w1 is:\n",
      "[[1.1638166]\n",
      " [1.0911828]] \n",
      "\n",
      "After 4000 training step, w1 is:\n",
      "[[1.1205218]\n",
      " [1.1888362]] \n",
      "\n",
      "After 4500 training step, w1 is:\n",
      "[[1.2550083]\n",
      " [1.2674794]] \n",
      "\n",
      "After 5000 training step, w1 is:\n",
      "[[1.085305 ]\n",
      " [1.1256213]] \n",
      "\n",
      "After 5500 training step, w1 is:\n",
      "[[1.0736042]\n",
      " [1.0458753]] \n",
      "\n",
      "After 6000 training step, w1 is:\n",
      "[[1.1864158]\n",
      " [1.0515581]] \n",
      "\n",
      "After 6500 training step, w1 is:\n",
      "[[1.1952157]\n",
      " [1.0819554]] \n",
      "\n",
      "After 7000 training step, w1 is:\n",
      "[[1.1945369]\n",
      " [1.0842199]] \n",
      "\n",
      "After 7500 training step, w1 is:\n",
      "[[1.1297575]\n",
      " [1.0491594]] \n",
      "\n",
      "After 8000 training step, w1 is:\n",
      "[[1.0211936]\n",
      " [1.078543 ]] \n",
      "\n",
      "After 8500 training step, w1 is:\n",
      "[[1.1529499]\n",
      " [1.0680149]] \n",
      "\n",
      "After 9000 training step, w1 is:\n",
      "[[1.1619881]\n",
      " [1.0830511]] \n",
      "\n",
      "After 9500 training step, w1 is:\n",
      "[[1.0654143]\n",
      " [1.141447 ]] \n",
      "\n",
      "Final w1 is:  [[1.1260294]\n",
      " [1.0665996]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.matmul(x, w1)\n",
    "        loss_zdy = tf.reduce_sum(tf.where(tf.greater(y,y_), (y-y_) * COST, (y_-y) * PROFIT))\n",
    "    grads = tape.gradient(loss_zdy, w1)\n",
    "    w1.assign_sub(lr * grads)\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(\"After {} training step, w1 is:\".format(epoch))\n",
    "        print(w1.numpy(), \"\\n\")  \n",
    "print(\"Final w1 is: \", w1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 交叉熵损失函数CE: tf.losses.categorical_crossentropy(y_, y)\n",
    "### 表征两个概率分布的距离，交叉熵越大两个概率分布越远，交叉熵越小两个概率分布越近。\n",
    "### 例题: 二分类 已知答案y_=(1, 0) 预测y1=(0.6, 0.4) y2=(0.8, 0.2) 哪个更接近标准答案？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_ce1: tf.Tensor(0.5108256, shape=(), dtype=float32)\n",
      "loss_ce2: tf.Tensor(0.22314353, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss_ce1 = tf.losses.categorical_crossentropy([1, 0], [0.6, 0.4])\n",
    "loss_ce2 = tf.losses.categorical_crossentropy([1, 0], [0.8, 0.2])\n",
    "print(\"loss_ce1:\", loss_ce1)\n",
    "print(\"loss_ce2:\", loss_ce2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. softmax与交叉熵结合: tf.nn.softmax_cross_entropy_with_logits(y_, y)\n",
    "### 输出先过softmax函数，在计算y与y_的交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00\n",
      " 5.49852354e-02], shape=(5,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00\n",
      " 5.49852354e-02], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "y_ = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0], [0,1,0]])\n",
    "y = np.array([[12,3,2], [3,10,1], [1,2,5], [4,6.5,1.2], [3,6,1]])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "loss_ce1 = tf.losses.categorical_crossentropy(y_, y_pro)\n",
    "loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)\n",
    "print(loss_ce1)\n",
    "print(loss_ce2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
